{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# initializing Pyspark\n",
        "%%time\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop2.tgz\n",
        "\n",
        "\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"spark-3.3.2-bin-hadoop2\")# SPARK_HOME\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "wR3ZcpOU0fIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a218c6e3-11f8-41be-a91d-ff6e5f374b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 203 ms, sys: 59.1 ms, total: 262 ms\n",
            "Wall time: 21.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Data read\n",
        "textFile = sc.textFile('/content/finalData.csv')\n",
        "count = textFile.count()\n",
        "print(count)\n",
        "rdd = textFile.map(lambda x: x.split(\",\")).flatMap(lambda x: [(x[0],x[i]) for i in range(1,count)])\n",
        "\n",
        "#rdd = textFile.map(lambda x: x.split(\",\")).map(lambda x: (x[0],1))\n",
        "\n",
        "print(rdd.collect())\n",
        "\n",
        "#creating dictionary with vertex and adjacency list\n",
        "lists = rdd.filter(lambda x: x[1]!='').groupByKey().map(lambda x: (x[0],list(x[1])))\n",
        "\n",
        "#only for debugging\n",
        "#print(lists.collect())"
      ],
      "metadata": {
        "id": "1AaF6aDg6Ci8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEzKDAAEzZNE"
      },
      "outputs": [],
      "source": [
        "keys = lists.keys().collect()\n",
        "values = lists.values().collect()\n",
        "\n",
        "data = dict(zip(keys,values))\n",
        "print(data)\n",
        "\n",
        "def Mapper_Pcss(v,nlist):\n",
        "  for vi in nlist:\n",
        "    if v<vi:\n",
        "      key = (v,vi)\n",
        "    else:\n",
        "      key = (vi,v)\n",
        "    value = nlist\n",
        "    yield (key,value)\n",
        "\n",
        "#When you call a transformation like map on an RDD, PySpark will automatically distribute the computation across the available nodes in the cluster.\n",
        "result = lists.flatMap(lambda x: Mapper_Pcss(x[0],x[1]))\n",
        "print(result.collect())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combiner\n",
        "def combiner_Pcss(r):\n",
        "  return r.groupByKey().map(lambda x: (x[0],list(x[1])))\n",
        "\n",
        "combiner1 = combiner_Pcss(result)\n",
        "print(combiner1.collect())"
      ],
      "metadata": {
        "id": "V1uj8yo2z47Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5e654c-fd1c-461c-e477-7435eb7be9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('0', '1'), [['1', '4'], ['0', '2', '3', '4']]), (('0', '4'), [['1', '4'], ['0', '1', '3']]), (('1', '4'), [['0', '2', '3', '4'], ['0', '1', '3']]), (('2', '3'), [['1', '3'], ['1', '2', '4']]), (('1', '2'), [['0', '2', '3', '4'], ['1', '3']]), (('1', '3'), [['0', '2', '3', '4'], ['1', '2', '4']]), (('3', '4'), [['0', '1', '3'], ['1', '2', '4']])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Key = combiner1.map(lambda m: m[0]).collect()\n",
        "l1 = combiner1.map(lambda m: m[1][0]).collect()\n",
        "l2 = combiner1.map(lambda m: m[1][1]).collect()\n",
        "epsilon = 0.25\n",
        "\n",
        "#Reducer\n",
        "def reducer_Pcss(Key,Adl1,Adl2):\n",
        "  similarity = Jack(Adl1,Adl2)\n",
        "  if similarity> epsilon:\n",
        "     return (Key,similarity)\n",
        "  #return (Key,similarity)\n",
        "\n",
        "#Calculating structural similarity\n",
        "def Jack(a,b):\n",
        "  a = set(tuple(x) for x in a)\n",
        "  b = set(tuple(x) for x in b)\n",
        "  intersection = float(len(a.intersection(b)))\n",
        "  union = float(len(a.union(b)))\n",
        "  #Only for dubugging\n",
        "  print(a,b)\n",
        "  print(\"Intersection:\", intersection, \"Union:\", union)\n",
        "  s = float(intersection)/float(union)\n",
        "  return s\n",
        "\n",
        "reducer_one = list(map(lambda X: reducer_Pcss(X[0], X[1], X[2]), zip(Key, l1, l2)))\n",
        "reducer1_rdd = sc.parallelize(reducer_one)\n",
        "print(reducer1_rdd.collect())"
      ],
      "metadata": {
        "id": "_FUow9pUnsnT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076a8f4f-88fd-4028-a5f3-046401b16229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('4',), ('1',)} {('2',), ('0',), ('4',), ('3',)}\n",
            "Intersection: 1.0 Union: 5.0\n",
            "{('4',), ('1',)} {('0',), ('1',), ('3',)}\n",
            "Intersection: 1.0 Union: 4.0\n",
            "{('2',), ('0',), ('4',), ('3',)} {('0',), ('1',), ('3',)}\n",
            "Intersection: 2.0 Union: 5.0\n",
            "{('1',), ('3',)} {('2',), ('4',), ('1',)}\n",
            "Intersection: 1.0 Union: 4.0\n",
            "{('2',), ('0',), ('4',), ('3',)} {('1',), ('3',)}\n",
            "Intersection: 1.0 Union: 5.0\n",
            "{('2',), ('0',), ('4',), ('3',)} {('2',), ('4',), ('1',)}\n",
            "Intersection: 2.0 Union: 5.0\n",
            "{('0',), ('1',), ('3',)} {('2',), ('4',), ('1',)}\n",
            "Intersection: 1.0 Union: 5.0\n",
            "[None, None, (('1', '4'), 0.4), None, None, (('1', '3'), 0.4), None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reducer_res = reducer1_rdd.filter(lambda x: x!=None)\n",
        "\n",
        "n1 = reducer_res.map(lambda x: (x[0][0],x[0][1])).groupByKey().map(lambda x: (x[0],list(x[1])))\n",
        "n2 = reducer_res.map(lambda x: (x[0][1],x[0][0])).groupByKey().map(lambda x: (x[0],list(x[1])))\n",
        "\n",
        "partial = n1.union(n2).collect()\n",
        "lpcc_network = sc.parallelize(partial)\n",
        "\n",
        "#finding outliers\n",
        "outliers = lists.keys().subtract(lpcc_network.keys())\n",
        "print(lpcc_network.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A2GxQK_ZPCu",
        "outputId": "35934f5b-dbf6-43ed-82bb-7bd073da5cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('1', ['4', '3']), ('4', ['1']), ('3', ['1'])]\n",
            "['0', '2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import builtins as __builtin__\n",
        "\n",
        "#class for managing the structure information of each node\n",
        "class Structure:\n",
        "  def __init__(self, status, label, adlist):\n",
        "    self.status = status\n",
        "    self.label = label\n",
        "    self.adlist = adlist\n",
        "\n",
        "  #for printing adjustment\n",
        "  def __repr__(self):\n",
        "    p = \"{Status: \" + str(self.status) + \", Label: \" + str(self.label) + \", List: \" + str(self.adlist) + \"}\"\n",
        "    return p\n",
        "  def __str__(self):\n",
        "    p = \"Status: \" + str(self.status) + \", Label: \" + str(self.label) + \", List: \" + self.adlist\n",
        "    return p"
      ],
      "metadata": {
        "id": "ooMxtHmuYfIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LPCC_Mapper(v,status,label,adlist):\n",
        "  if status==1:\n",
        "    for vi in adlist:\n",
        "      key = vi\n",
        "      value = label\n",
        "      yield (key,value)\n",
        "  key=v\n",
        "  value = Structure(status,label,adlist)\n",
        "  yield (key,value)"
      ],
      "metadata": {
        "id": "3Vqhwc6lgckx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aux_combiner(t):\n",
        "  labels = []\n",
        "  for elem in t[1]:\n",
        "    if type(elem)==Structure:\n",
        "      struc = elem\n",
        "    else:\n",
        "      labels.append(elem)\n",
        "  return (t[0],[struc,labels])\n",
        "\n",
        "def LPCC_combiner(r):\n",
        "  temp = r.groupByKey().map(lambda x: (x[0],list(x[1]))).map(lambda x: aux_combiner(x))\n",
        "  #r1 = r.filter(lambda x: type(x[1])==Structure).groupByKey().map(lambda x: (x[0],list(x[1])[0]))\n",
        "  return temp"
      ],
      "metadata": {
        "id": "G3o5CM3p4fGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def LPCC_Reducer(v,status,label,adlist,labels):\n",
        "  if len(labels)!=0:\n",
        "    lmin = int(labels[0])\n",
        "    for l in labels:\n",
        "      if int(l) < lmin:\n",
        "        lmin = int(l)\n",
        "    if lmin<int(label):\n",
        "      status = 1\n",
        "      label = lmin\n",
        "    else:\n",
        "      status = 0\n",
        "  else:\n",
        "    status = 0\n",
        "  key = v\n",
        "  value= Structure(status,label,adlist)\n",
        "  return (key,value)\n"
      ],
      "metadata": {
        "id": "sOYgrWsNBTX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LPCC(network):\n",
        "  graph = network.map(lambda x: (x[0],Structure(1,x[0],x[1])))\n",
        "\n",
        "  #only for debugging\n",
        "  print(graph.collect())\n",
        "\n",
        "  #counting the number of activated nodes\n",
        "  s = graph.map(lambda x: x[1].status).reduce(lambda x, y: x+y)\n",
        "\n",
        "  while(s>0):\n",
        "\n",
        "    m2 =graph.flatMap(lambda x: LPCC_Mapper(x[0],x[1].status,x[1].label,x[1].adlist))\n",
        "\n",
        "    print(\"After mapper: \")\n",
        "    print(m2.collect())\n",
        "\n",
        "    combined = LPCC_combiner(m2)\n",
        "\n",
        "    print(\"After combiner: \")\n",
        "    print(combined.collect())\n",
        "\n",
        "    #calling reducer\n",
        "    key = combined.flatMap(lambda x: x[0]).collect()\n",
        "    stat = combined.map(lambda x: x[1][0].status).collect()\n",
        "    label = combined.map(lambda x: x[1][0].label).collect()\n",
        "    adlist = combined.map(lambda x: x[1][0].adlist).collect()\n",
        "    labels = (combined.map(lambda x: x[1][1]).collect())\n",
        "    r2 = list(map(lambda x: LPCC_Reducer(x[0],x[1],x[2],x[3],x[4]),zip(key,stat,label,adlist,labels)))\n",
        "    graph = sc.parallelize(r2)\n",
        "\n",
        "    print(\"After reducer: \")\n",
        "    print(graph.collect())\n",
        "    s = graph.map(lambda x: x[1].status).reduce(lambda x, y: x+y)\n",
        "    print(s)\n",
        "\n",
        "  clusters = graph.map(lambda x: (int(x[1].label),x[0])).groupByKey().map(lambda x: (x[0],list(x[1])))\n",
        "  print(\"FINAL CLUSTERS: \")\n",
        "  print(clusters.collect())\n",
        "\n",
        "LPCC(lpcc_network)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaNkouy9Y42q",
        "outputId": "273925d9-2004-438b-e6c2-98e75fa46405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('1', {Status: 1, Label: 1, List: ['4', '3']}), ('4', {Status: 1, Label: 4, List: ['1']}), ('3', {Status: 1, Label: 3, List: ['1']})]\n",
            "After mapper: \n",
            "[('4', '1'), ('3', '1'), ('1', {Status: 1, Label: 1, List: ['4', '3']}), ('1', '4'), ('4', {Status: 1, Label: 4, List: ['1']}), ('1', '3'), ('3', {Status: 1, Label: 3, List: ['1']})]\n",
            "After combiner: \n",
            "[('4', [{Status: 1, Label: 4, List: ['1']}, ['1']]), ('1', [{Status: 1, Label: 1, List: ['4', '3']}, ['4', '3']]), ('3', [{Status: 1, Label: 3, List: ['1']}, ['1']])]\n",
            "After reducer: \n",
            "[('4', {Status: 1, Label: 1, List: ['1']}), ('1', {Status: 0, Label: 1, List: ['4', '3']}), ('3', {Status: 1, Label: 1, List: ['1']})]\n",
            "2\n",
            "After mapper: \n",
            "[('1', 1), ('4', {Status: 1, Label: 1, List: ['1']}), ('1', {Status: 0, Label: 1, List: ['4', '3']}), ('1', 1), ('3', {Status: 1, Label: 1, List: ['1']})]\n",
            "After combiner: \n",
            "[('1', [{Status: 0, Label: 1, List: ['4', '3']}, [1, 1]]), ('4', [{Status: 1, Label: 1, List: ['1']}, []]), ('3', [{Status: 1, Label: 1, List: ['1']}, []])]\n",
            "After reducer: \n",
            "[('1', {Status: 0, Label: 1, List: ['4', '3']}), ('4', {Status: 0, Label: 1, List: ['1']}), ('3', {Status: 0, Label: 1, List: ['1']})]\n",
            "0\n",
            "FINAL CLUSTERS: \n",
            "[(1, ['1', '4', '3'])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"OUTLIERS\")\n",
        "print(outliers.collect())"
      ],
      "metadata": {
        "id": "H4shuqoN-IDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37c7d59-5c42-482f-b420-7500ed0fa08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUTLIERS\n",
            "['0', '2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "3zPmPXaTjmTr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}